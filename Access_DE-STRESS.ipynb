{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the DE-STRESS Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imported .dump file with all DE-STRESS data into local Postgres server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_restore -U postgres -d local_DE-STRESS_DB file_path/bigstructure.dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracted wanted information from DE-STRESS into pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker, Session\n",
    "import multiprocessing\n",
    "import time\n",
    "import pickle\n",
    "from functools import partial\n",
    "from db_settings import URL\n",
    "from DB_library import ChainModel, StateModel, RosettaResultsModel, BiolUnitModel, PdbModel, BudeFFResultsModel,EvoEF2ResultsModel,DFIRE2ResultsModel,Aggrescan3DResultsModel\n",
    "from Rotatory_library import rotatory\n",
    "\n",
    "\n",
    "engine=create_engine( URL, convert_unicode=True,pool_pre_ping=True)\n",
    "sessions=sessionmaker()\n",
    "session=sessions(bind=engine)\n",
    "\n",
    "def querying(list_of_datapoints, iteration_index):   # Function for  querying through multiprocessing instead of a for loop\n",
    "    \n",
    "    if iteration_index<len(list_of_datapoints):         # Prevents an 'IndexError: list index out of range' because when query slides to the last window it will iterate over 10 000 even though that query.all() list is smaller.\n",
    "        instance=list_of_datapoints[iteration_index]    # This instance is a list where each element is an object of each table queried - query(ChainModel, StateModel,RosettaResultsModel, BiolUnitModel,PdbModel )\n",
    "        dict_of_attributes={'Chain ID':instance[0].id, 'Chain State':instance[1].state_number,\n",
    "        'Mean Pack Density':instance[1].mean_packing_density,'Chain Length':len(instance[0].sequence),\n",
    "        'Rotatory Bonds':rotatory(instance[0].sequence),'Rosetta Energy': instance[2].total_score, \n",
    "        'PDB Code':instance[4].pdb_code, 'Exp Method':instance[4].method, 'Bude Score':instance[5].total_energy,\n",
    "         'Evo Score':instance[6].total, 'DFire Score':instance[7].total, 'Aggre Score':instance[8].total_value}\n",
    "        \n",
    "        return dict_of_attributes\n",
    "\n",
    "whole_data_list=[]          # List of dictionaries, where each dictionary is an instance with each attribute as a key, and correspondent value\n",
    "if __name__=='__main__':                 \n",
    "    query=session.query(ChainModel, StateModel,RosettaResultsModel, BiolUnitModel,\n",
    "    PdbModel).distinct(ChainModel.sequence).join(ChainModel.state).join(\n",
    "    StateModel.biol_unit).join(BiolUnitModel.pdb).join(StateModel.rosetta_results)\n",
    "    \n",
    "    for number in range((query.count()//10000)+1):  # query object allows to get the length of the query. The .count() method does not work if query.all()\n",
    "        \n",
    "        with Session(engine) as session:            # Use with python context manager to separate the transactions between the queries of each iteration of the for loop above\n",
    "            data=session.query(ChainModel, StateModel,RosettaResultsModel, \n",
    "                BiolUnitModel,PdbModel,BudeFFResultsModel,EvoEF2ResultsModel,\n",
    "                DFIRE2ResultsModel,Aggrescan3DResultsModel).distinct(ChainModel.sequence).join(\n",
    "                ChainModel.state).join(StateModel.biol_unit).join(BiolUnitModel.pdb).join(\n",
    "                StateModel.rosetta_results).join(StateModel.budeff_results).join(\n",
    "                StateModel.evoef2_results).join(StateModel.dfire2_results).join(\n",
    "                StateModel.aggrescan3d_results).offset(number*10000).limit(10000).all() # Distinct filters the chains so that they are all uniq\n",
    "            \n",
    "            with multiprocessing.Pool(7) as pool:\n",
    "                sliced_data=pool.map(partial(querying, data), range(10000))  # map takes one function and one list, and partial allows to add the data object as a constant.\n",
    "                whole_data_list+=sliced_data                                 # pool joins the outcome of each iteration within the sliced data into a list (sliced_data). This creates a list of dictionaries of the size of the slice section\n",
    "\n",
    "    with open('DE-STRESS_data.pickle','wb') as pickle_out: \n",
    "        pickle.dump(whole_data_list, pickle_out)                            # Extracted data saved in the 'DE-STRESS_data' pickle file\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcc9d14f13fdb1a17c1463029b9ae143e53bd3aa78ed9c0acd3ffe70e799f014"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
